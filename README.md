# Prusti Performance Monitoring & Benchmarking

This repository contains two primary crates: 
* [`collector`](./collector): gathers data for each bors commit
* [`site`](./site): [displays](http://34.228.27.164:2346) the data and provides a GitHub bot for on-demand benchmarking

Additional documentation on running and setting up the frontend and backend can
be found in the `README` files in the `collector` and `site` directories.

Additional documentation on the benchmark programs can be found in the `README`
file in the `collector/benchmarks` directory.

## Quick Start

1. Run the script `scripts/setup_aws.sh` to install necessary dependencies
2. Run the database with the command `scripts/start_db.sh`
3. Run the script `scripts/generate_prusti_benchs.sh` to benchmark all BORS commits from Prusti
4. Decide on a secret string that will be used to communicate with github webhooks
5. To run the server, run the command `env GITHUB_WEBHOOK_SECRET=[YOUR SECRET] scripts/run_site.sh` from this directory

## Testing a single commit locally

1. In the `prusti-dev` directory, checkout the commit
2. To ensure you are using the correct version of the viper tools, run `./x.py setup`
3. From the `prusti-perf` run the command `scripts/run_benchmark.sh`

### Setting up Github Integration

1. Go to the new webhook page: https://github.com/viperproject/prusti-dev/settings/hooks/new
2. Create a new webhook with the following settings:
  1. Under "payload URL" specify http://[SERVER ADDRESS]:2346/perf/github-hook
  2. Content type should be "applicatin/json"
  3. Trigger events should be "send me everything"
  4. Set the secret to the secret you decided above
3. Create an API token

## Adding Benchmarks

Benchmarks should be placed in the `collector/benchmarks` folder. Ensure that
there is a `Cargo.lock` file in the benchmark; this can be generated by running
the `cargo build` command.

## Notes

Currently all benchmarks use Z3 version 4.8.6.

10 Iterations, still yields ~2% difference on consecutive runs.

Excluding Viper parts from perf (by running in server) yields ~0.02% difference on 3 iterations!

## TODO

- [ ] Use Viperserver for more consistent results
- [ ] Remove first iteration of each run from statistics
- [ ] Github Integration
- [ ] Warnings for performance regressions
- [ ] Support comparison of different variants of Prusti (i.e., using different configuration parameters)
- [ ] Also include less interesting tests
- [ ] Also include very long tests
- [ ] Place those tests in their own categories

