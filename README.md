# Prusti Performance Monitoring & Benchmarking

This repository contains two primary crates: 
* [`collector`](./collector): gathers data for each bors commit
* [`site`](./site): [displays](http://3.94.193.1:2346) the data and provides a GitHub bot for on-demand benchmarking

Additional documentation on running and setting up the frontend and backend can
be found in the `README` files in the `collector` and `site` directories.

Additional documentation on the benchmark programs can be found in the `README`
file in the `collector/benchmarks` directory.

## Getting Started

### Hosting

TLDR: Should work out-of-the box using Ubuntu 22.04 LTS on AWS A1 instances
(both virtualized and non-virtualized). Using a bare-metal or dedicated A1
instance should give the best results.

Benchmarks are collected using the `perf` tool (although in principle other tools
are supported). `perf` collects various metrics; for some, hardware performance
monitors are required. These monitors are often not available on VPS, however
they __are__ available on virtualized Amazon ARM instances. Nonetheless, the
"wall-time" measurements do not require these monitors.

Typically, using the "instructions" metric from `perf` would yield the most
consistent results; compared to wall-time that would be impacted by other
processes, hardware changes etc. However, JVM caching / warm-up things are
highly variable across runs in terms of instruction counts. To address this, the
benchmarks are first run through once to warm up the Viper server, so that
subsequent runs have a hot cache (and can be benchmarked). However as it doesn't
seem trivial to capture the viper server instructions after the warmup in perf,
the "instructions" metric only counts those from Prusti. Thus, they do will not
capture some meaningful results (i.e. changes that make the viper encoding more
efficient). For this reason, wall-time measurements are used instead as the default.

### Instructions

#### 1. Run the script `scripts/setup_aws.sh` to install necessary dependencies
#### 2. Run the database with the command `scripts/start_db.sh`
#### 3. Setup Github Webhook
  0. Decide on a secret string for webhooks
  1. Go to the new webhook page: https://github.com/viperproject/prusti-dev/settings/hooks/new
  2. Create a new webhook with the following settings:
    1. Under "payload URL" specify http://[SERVER ADDRESS]:2346/perf/github-hook
    2. Content type should be "application/json"
    3. Trigger events should be "Issue Comments" and "Pushes"
    4. Set the secret to the secret you decided above
#### 4. Create a GitHub API token
  Instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token

#### 5. Run the site and collector scripts

  These two scripts will run persistently. A nice way to do this is to run them
  in seperate `tmux` panes (I think `screen` should work too?)
  
  For the server, you will need to export the Github Webhook Secret and API
  token. One way to do this is to create a file called `.env` that looks like:

``` sh
export GITHUB_WEBHOOK_SECRET=[The secret]
export GITHUB_API_TOKEN=[The secret]
```

  Then the "site" can be run with the following two commands

``` sh
source .env
scripts/run_site.sh
```

  The collector can be run with the command:

``` sh
scripts/run_collector.sh
```

## Testing a single commit locally

If you just want to test locally, complete the steps in the "Getting Started"
section, with a few changes:
1. It's not necessary to setup any of the Github stuff
2. Do __not__ run the collector script
3. You should run `cargo build` from the `collector` directory

Then:

1. In the `prusti-dev` directory, checkout the commit you wish to benchmark
2. To ensure you are using the correct version of the viper tools, run `./x.py setup`
3. From the `prusti-perf` run the command `scripts/run_benchmark.sh`

## Adding Benchmarks

Benchmarks should be placed in the `collector/benchmarks` folder. Ensure that
there is a `Cargo.lock` file in the benchmark; this can be generated by running
the `cargo build` command.

## Notes

Currently all benchmarks use Z3 version 4.8.6.

10 Iterations, still yields ~2% difference on consecutive runs.

Excluding Viper parts from perf (by running in server) yields ~0.02% difference on 3 iterations!

## TODO

- [ ] Support for entire Prusti test suite
- [ ] Warnings for performance regressions
- [ ] Support comparison of different variants of Prusti (i.e., using different configuration parameters)
- [ ] Also include less interesting tests
- [ ] Also include very long tests
- [ ] Place those tests in their own categories

